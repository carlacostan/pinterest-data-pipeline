{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fdb19f1-33f1-49fc-b337-0d4ed711fcab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1c35be3-a961-4549-a891-d0ab75a2f978",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32376d89-5e40-45b9-bf7c-9a22ed82e50d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>spark.databricks.delta.formatCheck.enabled</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "spark.databricks.delta.formatCheck.enabled",
         "false"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "key",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "--Disable format checks during the reading of Delta tables\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87951940-6d8b-4055-ae9a-460340dbb747",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data streaming\n",
    "\n",
    "# Pin data stream\n",
    "df_pin = spark.readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', 'streaming-12d03c8b5ccd-pin') \\\n",
    "    .option('initialPosition', 'earliest') \\\n",
    "    .option('region', 'us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "\n",
    "# Geo data stream\n",
    "df_geo = spark.readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', 'streaming-12d03c8b5ccd-geo') \\\n",
    "    .option('initialPosition', 'earliest') \\\n",
    "    .option('region', 'us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "\n",
    "# User data stream\n",
    "df_user = spark.readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', 'streaming-12d03c8b5ccd-user') \\\n",
    "    .option('initialPosition', 'earliest') \\\n",
    "    .option('region', 'us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bb1d87b-f514-48b7-be9a-4b85ce3ae687",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"/Workspace/Users/carla.costan0@gmail.com/data_cleaning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4503e97c-e734-44cd-9030-b2a740234b4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-1987352838955007&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\"># Deserialise the &#39;data&#39; column</span>\n",
       "<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>df_pin <span class=\"ansi-blue-fg\">=</span> df_pin<span class=\"ansi-blue-fg\">.</span>selectExpr<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;CAST(data as STRING)&#34;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\"># Apply schema and clean pin data</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">selectExpr</span><span class=\"ansi-blue-fg\">(self, *expr)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1841</span>         <span class=\"ansi-green-fg\">if</span> len<span class=\"ansi-blue-fg\">(</span>expr<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">1</span> <span class=\"ansi-green-fg\">and</span> isinstance<span class=\"ansi-blue-fg\">(</span>expr<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> list<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1842</span>             expr <span class=\"ansi-blue-fg\">=</span> expr<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1843</span><span class=\"ansi-red-fg\">         </span>jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>selectExpr<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jseq<span class=\"ansi-blue-fg\">(</span>expr<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1844</span>         <span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>jdf<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1845</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n",
       "\n",
       "<span class=\"ansi-red-fg\">AnalysisException</span>: Column &#39;data&#39; does not exist. Did you mean one of the following? [ind, title, category, tag_list, image_src, description, poster_name, unique_id, save_location, follower_count, is_image_or_video]; line 1 pos 5;\n",
       "&#39;Project [unresolvedalias(cast(&#39;data as string), None)]\n",
       "+- Project [pin_data#17584588.ind AS ind#17584590, pin_data#17584588.unique_id AS unique_id#17584591, pin_data#17584588.title AS title#17584592, pin_data#17584588.description AS description#17584593, pin_data#17584588.follower_count AS follower_count#17584594, pin_data#17584588.poster_name AS poster_name#17584595, pin_data#17584588.tag_list AS tag_list#17584596, pin_data#17584588.is_image_or_video AS is_image_or_video#17584597, pin_data#17584588.image_src AS image_src#17584598, pin_data#17584588.save_location AS save_location#17584599, pin_data#17584588.category AS category#17584600]\n",
       "   +- Project [from_json(StructField(ind,IntegerType,true), StructField(unique_id,StringType,true), StructField(title,StringType,true), StructField(description,StringType,true), StructField(follower_count,StringType,true), StructField(poster_name,StringType,true), StructField(tag_list,StringType,true), StructField(is_image_or_video,StringType,true), StructField(image_src,StringType,true), StructField(save_location,StringType,true), StructField(category,StringType,true), data#17584380, Some(Etc/UTC)) AS pin_data#17584588]\n",
       "      +- Project [cast(data#17584333 as string) AS data#17584380]\n",
       "         +- StreamingRelationV2 com.databricks.sql.kinesis.KinesisSourceProvider@143de000, kinesis, com.databricks.sql.kinesis.KinesisTable@405e0150, [initialPosition=earliest, awsSecretKey=*********(redacted), region=us-east-1, awsAccessKey=AKIAYQJJUMTWXXGSLEMG, streamName=streaming-12d03c8b5ccd-pin], [partitionKey#17584332, data#17584333, stream#17584334, shardId#17584335, sequenceNumber#17584336, approximateArrivalTimestamp#17584337]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1987352838955007&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\"># Deserialise the &#39;data&#39; column</span>\n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>df_pin <span class=\"ansi-blue-fg\">=</span> df_pin<span class=\"ansi-blue-fg\">.</span>selectExpr<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;CAST(data as STRING)&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> <span class=\"ansi-red-fg\"># Apply schema and clean pin data</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">selectExpr</span><span class=\"ansi-blue-fg\">(self, *expr)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1841</span>         <span class=\"ansi-green-fg\">if</span> len<span class=\"ansi-blue-fg\">(</span>expr<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">1</span> <span class=\"ansi-green-fg\">and</span> isinstance<span class=\"ansi-blue-fg\">(</span>expr<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> list<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1842</span>             expr <span class=\"ansi-blue-fg\">=</span> expr<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-fg\">-&gt; 1843</span><span class=\"ansi-red-fg\">         </span>jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>selectExpr<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jseq<span class=\"ansi-blue-fg\">(</span>expr<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1844</span>         <span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>jdf<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1845</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: Column &#39;data&#39; does not exist. Did you mean one of the following? [ind, title, category, tag_list, image_src, description, poster_name, unique_id, save_location, follower_count, is_image_or_video]; line 1 pos 5;\n&#39;Project [unresolvedalias(cast(&#39;data as string), None)]\n+- Project [pin_data#17584588.ind AS ind#17584590, pin_data#17584588.unique_id AS unique_id#17584591, pin_data#17584588.title AS title#17584592, pin_data#17584588.description AS description#17584593, pin_data#17584588.follower_count AS follower_count#17584594, pin_data#17584588.poster_name AS poster_name#17584595, pin_data#17584588.tag_list AS tag_list#17584596, pin_data#17584588.is_image_or_video AS is_image_or_video#17584597, pin_data#17584588.image_src AS image_src#17584598, pin_data#17584588.save_location AS save_location#17584599, pin_data#17584588.category AS category#17584600]\n   +- Project [from_json(StructField(ind,IntegerType,true), StructField(unique_id,StringType,true), StructField(title,StringType,true), StructField(description,StringType,true), StructField(follower_count,StringType,true), StructField(poster_name,StringType,true), StructField(tag_list,StringType,true), StructField(is_image_or_video,StringType,true), StructField(image_src,StringType,true), StructField(save_location,StringType,true), StructField(category,StringType,true), data#17584380, Some(Etc/UTC)) AS pin_data#17584588]\n      +- Project [cast(data#17584333 as string) AS data#17584380]\n         +- StreamingRelationV2 com.databricks.sql.kinesis.KinesisSourceProvider@143de000, kinesis, com.databricks.sql.kinesis.KinesisTable@405e0150, [initialPosition=earliest, awsSecretKey=*********(redacted), region=us-east-1, awsAccessKey=AKIAYQJJUMTWXXGSLEMG, streamName=streaming-12d03c8b5ccd-pin], [partitionKey#17584332, data#17584333, stream#17584334, shardId#17584335, sequenceNumber#17584336, approximateArrivalTimestamp#17584337]\n</div>",
       "errorSummary": "<span class=\"ansi-red-fg\">AnalysisException</span>: Column &#39;data&#39; does not exist. Did you mean one of the following? [ind, title, category, tag_list, image_src, description, poster_name, unique_id, save_location, follower_count, is_image_or_video]; line 1 pos 5;",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean the data\n",
    "\n",
    "# Deserialise the 'data' column\n",
    "df_pin = df_pin.selectExpr(\"CAST(data as STRING)\")\n",
    "\n",
    "# Apply schema and clean pin data\n",
    "pin_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType(), True),\n",
    "    StructField(\"unique_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"follower_count\", StringType(), True),\n",
    "    StructField(\"poster_name\", StringType(), True),\n",
    "    StructField(\"tag_list\", StringType(), True),\n",
    "    StructField(\"is_image_or_video\", StringType(), True),\n",
    "    StructField(\"image_src\", StringType(), True),\n",
    "    StructField(\"save_location\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_pin = df_pin.select(from_json(col(\"data\"), pin_schema).alias(\"pin_data\")).select(\"pin_data.*\")\n",
    "cleaned_df_pin = clean_df_pin(df_pin)\n",
    "\n",
    "# Deserialise the 'data' column\n",
    "df_geo = df_geo.selectExpr(\"CAST(data as STRING)\")\n",
    "\n",
    "# Apply schema and clean geo data\n",
    "geo_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_geo = df_geo.select(from_json(col(\"data\"), geo_schema).alias(\"geo_data\")).select(\"geo_data.*\")\n",
    "cleaned_df_geo = clean_df_geo(df_geo)\n",
    "\n",
    "# Deserialise the 'data' column\n",
    "df_user = df_user.selectExpr(\"CAST(data as STRING)\")\n",
    "\n",
    "# Apply schema and clean user data\n",
    "user_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"date_joined\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_user = df_user.select(from_json(col(\"data\"), user_schema).alias(\"user_data\")).select(\"user_data.*\")\n",
    "cleaned_df_user = clean_df_user(df_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cef29171-92ff-41d8-a1a9-0a05baadab7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write cleaned data to the Delta table\n",
    "\n",
    "# Pin data\n",
    "cleaned_df_pin.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints_pin/\") \\\n",
    "    .table(\"12d03c8b5ccd_pin_table\") \n",
    "\n",
    "# Geo data\n",
    "cleaned_df_geo.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints_geo/\") \\\n",
    "    .table(\"12d03c8b5ccd_geo_table\")\n",
    "\n",
    "# User data\n",
    "cleaned_df_user.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints_user/\") \\\n",
    "    .table(\"12d03c8b5ccd_user_table\")\n",
    "\n",
    "# Step 10: Clean up checkpoint folders before re-running (only if necessary)\n",
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints_pin/\", True)\n",
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints_geo/\", True)\n",
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints_user/\", True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1987352838955006,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "stream_clean_write_kinesis_data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
