{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fdb19f1-33f1-49fc-b337-0d4ed711fcab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1c35be3-a961-4549-a891-d0ab75a2f978",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32376d89-5e40-45b9-bf7c-9a22ed82e50d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Disable format checks during the reading of Delta tables\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87951940-6d8b-4055-ae9a-460340dbb747",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data streaming\n",
    "\n",
    "# Pin data stream\n",
    "df_pin = spark.readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', 'streaming-12d03c8b5ccd-pin') \\\n",
    "    .option('initialPosition', 'earliest') \\\n",
    "    .option('region', 'us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "\n",
    "# Geo data stream\n",
    "df_geo = spark.readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', 'streaming-12d03c8b5ccd-geo') \\\n",
    "    .option('initialPosition', 'earliest') \\\n",
    "    .option('region', 'us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "\n",
    "# User data stream\n",
    "df_user = spark.readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', 'streaming-12d03c8b5ccd-user') \\\n",
    "    .option('initialPosition', 'earliest') \\\n",
    "    .option('region', 'us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bb1d87b-f514-48b7-be9a-4b85ce3ae687",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/carla.costan0@gmail.com/data_cleaning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4503e97c-e734-44cd-9030-b2a740234b4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "\n",
    "# Deserialise the 'data' column\n",
    "df_pin = df_pin.selectExpr(\"CAST(data as STRING)\")\n",
    "\n",
    "# Apply schema and clean pin data\n",
    "pin_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType(), True),\n",
    "    StructField(\"unique_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"follower_count\", StringType(), True),\n",
    "    StructField(\"poster_name\", StringType(), True),\n",
    "    StructField(\"tag_list\", StringType(), True),\n",
    "    StructField(\"is_image_or_video\", StringType(), True),\n",
    "    StructField(\"image_src\", StringType(), True),\n",
    "    StructField(\"save_location\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_pin = df_pin.select(from_json(col(\"data\"), pin_schema).alias(\"pin_data\")).select(\"pin_data.*\")\n",
    "cleaned_df_pin = clean_df_pin(df_pin)\n",
    "\n",
    "# Deserialise the 'data' column\n",
    "df_geo = df_geo.selectExpr(\"CAST(data as STRING)\")\n",
    "\n",
    "# Apply schema and clean geo data\n",
    "geo_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_geo = df_geo.select(from_json(col(\"data\"), geo_schema).alias(\"geo_data\")).select(\"geo_data.*\")\n",
    "cleaned_df_geo = clean_df_geo(df_geo)\n",
    "\n",
    "# Deserialise the 'data' column\n",
    "df_user = df_user.selectExpr(\"CAST(data as STRING)\")\n",
    "\n",
    "# Apply schema and clean user data\n",
    "user_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"date_joined\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_user = df_user.select(from_json(col(\"data\"), user_schema).alias(\"user_data\")).select(\"user_data.*\")\n",
    "cleaned_df_user = clean_df_user(df_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cef29171-92ff-41d8-a1a9-0a05baadab7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write cleaned data to the Delta table\n",
    "\n",
    "# Pin data\n",
    "cleaned_df_pin.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints_pin/\") \\\n",
    "    .table(\"12d03c8b5ccd_pin_table\") \n",
    "\n",
    "# Geo data\n",
    "cleaned_df_geo.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints_geo/\") \\\n",
    "    .table(\"12d03c8b5ccd_geo_table\")\n",
    "\n",
    "# User data\n",
    "cleaned_df_user.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints_user/\") \\\n",
    "    .table(\"12d03c8b5ccd_user_table\")\n",
    "\n",
    "# Step 10: Clean up checkpoint folders before re-running (only if necessary)\n",
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints_pin/\", True)\n",
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints_geo/\", True)\n",
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints_user/\", True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1987352838955006,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "stream_clean_write_kinesis_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
