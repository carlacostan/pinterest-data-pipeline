{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "349fc649-b601-4b9d-ae5a-403dbfceea6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6c7cc5e-6976-4e4f-8bdc-bd2bb0918e6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean df_pin\n",
    "def clean_df_pin(df_pin: DataFrame) -> DataFrame:\n",
    "    # Replace empty or irrelevant entries with None\n",
    "    markers_to_replace = [\"\", \"No description available\", \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\"]\n",
    "    for column_name in df_pin.columns:\n",
    "        df_pin = df_pin.withColumn(column_name, when(col(column_name).isin(markers_to_replace) | col(column_name).isNull(), None).otherwise(col(column_name)))\n",
    "\n",
    "    # Clean and convert follower_count to integer\n",
    "    df_pin = df_pin.withColumn(\"follower_count\",\n",
    "                               when(col(\"follower_count\").contains(\"k\"), (regexp_replace(col(\"follower_count\"), \"k\", \"\").cast(\"float\") * 1000))\n",
    "                               .when(col(\"follower_count\").contains(\"M\"), (regexp_replace(col(\"follower_count\"), \"M\", \"\").cast(\"float\") * 1000000))\n",
    "                               .otherwise(col(\"follower_count\").cast(\"float\")))\n",
    "\n",
    "    # Ensure 'downloaded' and 'index' columns are numeric\n",
    "    df_pin = df_pin.withColumn(\"downloaded\", col(\"downloaded\").cast(\"int\"))\n",
    "    df_pin = df_pin.withColumn(\"index\", col(\"index\").cast(\"int\"))\n",
    "\n",
    "    # Clean 'save_location' column to keep only the location path\n",
    "    df_pin = df_pin.withColumn(\"save_location\", regexp_replace(col(\"save_location\"), r\"https?://[^/]+/\", \"\"))\n",
    "\n",
    "    # Rename 'index' column to 'ind'\n",
    "    df_pin = df_pin.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "    # Reorder the DataFrame columns\n",
    "    column_order = [\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \n",
    "                    \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \n",
    "                    \"save_location\", \"category\"]\n",
    "\n",
    "    # Remove duplicates\n",
    "    df_pin = df_pin.dropDuplicates([\"ind\", \"category\"])\n",
    "\n",
    "    df_pin = df_pin.select(column_order)\n",
    "\n",
    "    return df_pin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f248727e-4e9b-439d-ad4f-8b80aa59b337",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean df_geo\n",
    "def clean_df_geo(df_geo: DataFrame) -> DataFrame:\n",
    "    # Create 'coordinates' array column\n",
    "    df_geo = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "\n",
    "    # Drop latitude and longitude columns\n",
    "    df_geo = df_geo.drop(\"latitude\", \"longitude\")\n",
    "\n",
    "    # Convert 'timestamp' to a proper timestamp data type\n",
    "    df_geo = df_geo.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "    # Ensure 'ind' is numeric\n",
    "    df_geo = df_geo.withColumn(\"ind\", col(\"ind\").cast(\"int\"))\n",
    "\n",
    "    # Reorder the DataFrame columns\n",
    "    column_order = [\"ind\", \"country\", \"coordinates\", \"timestamp\"]\n",
    "\n",
    "    # Remove duplicates\n",
    "    df_geo = df_geo.dropDuplicates([\"ind\", \"country\"])\n",
    "\n",
    "    df_geo = df_geo.select(column_order)\n",
    "\n",
    "    return df_geo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be572661-5644-455f-8a2a-5f8f9bcf7647",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_df_geo(df_geo: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the geo DataFrame by creating a coordinates array and formatting the timestamp.\n",
    "    \n",
    "    Parameters:\n",
    "    df_geo (DataFrame): Spark DataFrame to be cleaned.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Cleaned Spark DataFrame.\n",
    "    \"\"\"\n",
    "    # Create 'coordinates' array column\n",
    "    df_geo = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "\n",
    "    # Drop latitude and longitude columns\n",
    "    df_geo = df_geo.drop(\"latitude\", \"longitude\")\n",
    "\n",
    "    # Convert 'timestamp' to a proper timestamp data type\n",
    "    df_geo = df_geo.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "    # Ensure 'ind' is numeric\n",
    "    df_geo = df_geo.withColumn(\"ind\", col(\"ind\").cast(\"int\"))\n",
    "\n",
    "    # Reorder the DataFrame columns\n",
    "    column_order = [\"ind\", \"country\", \"coordinates\", \"timestamp\"]\n",
    "    df_geo = df_geo.select(column_order)\n",
    "\n",
    "    return df_geo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b85b11d-b8b2-488e-a879-613a03210450",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_df_user(df_user: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the user DataFrame by concatenating first and last names and formatting the date.\n",
    "    \n",
    "    Parameters:\n",
    "    df_user (DataFrame): Spark DataFrame to be cleaned.\n",
    "    a\n",
    "    Returns:\n",
    "    DataFrame: Cleaned Spark DataFrame.\n",
    "    \"\"\"\n",
    "    # Concatenate 'first_name' and 'last_name' into 'user_name'\n",
    "    df_user = df_user.withColumn(\"user_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "\n",
    "    # Drop the 'first_name' and 'last_name' columns\n",
    "    df_user = df_user.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "    # Convert 'date_joined' to a proper timestamp data type\n",
    "    df_user = df_user.withColumn(\"date_joined\", to_timestamp(col(\"date_joined\")))\n",
    "\n",
    "    # Ensure 'ind' and 'age' are numeric\n",
    "    df_user = df_user.withColumn(\"ind\", col(\"ind\").cast(\"int\"))\n",
    "    df_user = df_user.withColumn(\"age\", col(\"age\").cast(\"int\"))\n",
    "\n",
    "    # Reorder the DataFrame columns\n",
    "    column_order = [\"ind\", \"user_name\", \"age\", \"date_joined\"]\n",
    "    df_user = df_user.select(column_order)\n",
    "    \n",
    "    return df_user"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_cleaning",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
